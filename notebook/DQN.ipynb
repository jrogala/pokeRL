{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pokerl.tools.devicehandler import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "STEP_LIMIT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.wrappers import GrayScaleObservation, ResizeObservation, TimeLimit\n",
    "\n",
    "from pokerl.env.pokemonblue import PokemonBlueEnv\n",
    "from pokerl.env.wrappers import (\n",
    "    ObservationAddPokemonLevel,\n",
    "    ObservationAddPosition,\n",
    "    ObservationDict,\n",
    "    RewardDecreasingNoChange,\n",
    "    RewardDecreasingSteps,\n",
    "    RewardHistoryToInfo,\n",
    "    RewardIncreasingBadges,\n",
    "    RewardIncreasingCapturePokemon,\n",
    "    RewardIncreasingPokemonLevel,\n",
    "    RewardIncreasingPositionExploration,\n",
    "    ppFlattenInfo,\n",
    ")\n",
    "\n",
    "\n",
    "def create_env() -> Env:\n",
    "    env = PokemonBlueEnv()\n",
    "    # Setting observation\n",
    "    env = ResizeObservation(env, 64)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ObservationDict(env)\n",
    "    env = ObservationAddPosition(env)\n",
    "    env = ObservationAddPokemonLevel(env)\n",
    "    # Setting reward\n",
    "    env = RewardDecreasingNoChange(env, 10)\n",
    "    env = RewardDecreasingSteps(env, 0.001)\n",
    "    env = RewardIncreasingBadges(env, 100)\n",
    "    env = RewardIncreasingCapturePokemon(env, 10)\n",
    "    env = RewardIncreasingPokemonLevel(env, 10)\n",
    "    env = RewardIncreasingPositionExploration(env, 1)\n",
    "    env = RewardHistoryToInfo(env)\n",
    "    # Post processing\n",
    "    env = TimeLimit(env, STEP_LIMIT)\n",
    "    env = ppFlattenInfo(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "Transitions = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transitions(state, action, next_state, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.memory[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # Screen is size 64x64x3\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model_screen = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1),  # Adjusted input channels\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0, -1),\n",
    "        )\n",
    "        self.model_info = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),  # Adjusted input dimension\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # self.model merge both models and output n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3136, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions),\n",
    "        )\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        screen, info = torch.split(x, [4096, 8], dim=1)\n",
    "\n",
    "        # screen is a batch of 64x64 images\n",
    "        screen = screen.view(batch_size, 1, 64, 64)\n",
    "        screen = self.model_screen(screen)\n",
    "\n",
    "        info = self.model_info(info)\n",
    "\n",
    "        tobecat = (screen.view(batch_size, -1), info.view(batch_size, -1))\n",
    "        x = torch.cat(tobecat, dim=1)\n",
    "\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    env: Env\n",
    "    device: str = \"\"\n",
    "\n",
    "    memory = ReplayMemory(STEP_LIMIT)\n",
    "\n",
    "    policy_net: DQN = field(init=False)\n",
    "    target_net: DQN = field(init=False)\n",
    "    optimizer: optim.AdamW = field(init=False)\n",
    "    last_state: torch.Tensor = field(init=False)\n",
    "    eps: float = 0.3\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device == \"\":\n",
    "            self.device = get_device()\n",
    "        n_actions = self.env.action_space.n\n",
    "        self.policy_net = DQN(n_actions).to(self.device)\n",
    "        self.target_net = DQN(n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "        self.last_state = self._get_state(self.env.observation_space.sample()).to(\"cpu\")\n",
    "\n",
    "    def _get_state(self, observation) -> torch.Tensor:\n",
    "        return torch.cat(\n",
    "            (\n",
    "                torch.tensor(observation[\"screen\"], dtype=torch.float).view(-1).unsqueeze(0),\n",
    "                torch.tensor(observation[\"info\"], dtype=torch.float).view(-1).unsqueeze(0),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def select_action(self, observation, info):\n",
    "        state = self._get_state(observation).to(self.device)\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps\n",
    "        info[\"tick\"] += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=self.device, dtype=torch.long)\n",
    "\n",
    "    def step(self, observation, reward, done, action_int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            action_int (_type_): action to take\n",
    "\n",
    "        Returns:\n",
    "            bool: False if the episode is done\n",
    "        \"\"\"\n",
    "        state = self._get_state(observation).clone().detach().cpu()\n",
    "        self.memory.push(\n",
    "            self.last_state.to(\"cpu\"),\n",
    "            torch.tensor(action_int, dtype=torch.long).to(\"cpu\"),\n",
    "            state.to(\"cpu\"),\n",
    "            torch.tensor(reward, dtype=torch.float).to(\"cpu\"),\n",
    "            torch.tensor(done, dtype=torch.bool).to(\"cpu\"),\n",
    "        )\n",
    "        self.last_state = state\n",
    "\n",
    "    def optimize_model(self, amount_of_optimizations=1):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        losses = []\n",
    "        for _ in tqdm(range(amount_of_optimizations)):\n",
    "            transitions = self.memory.sample(BATCH_SIZE)\n",
    "            batch = Transitions(*zip(*transitions))\n",
    "            state_batch = torch.cat(batch.state).to(self.device)\n",
    "            action_batch = torch.tensor(np.array(batch.action)).squeeze(1).squeeze(1).to(self.device)\n",
    "            action_batch_hot = F.one_hot(action_batch, self.env.action_space.n).to(self.device)\n",
    "            reward_batch = torch.tensor(np.array(batch.reward)).to(self.device)\n",
    "            next_state_batch = torch.cat(batch.next_state).to(self.device)\n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch_hot)\n",
    "            next_state_values = torch.zeros(BATCH_SIZE)\n",
    "            with torch.no_grad():\n",
    "                next_state_values = self.target_net(next_state_batch).max(1).values\n",
    "            expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "            criterion = F.mse_loss\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_play(local_env, local_agent):\n",
    "    # Initialize the environment and get its state\n",
    "    observation, info = local_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_int = local_agent.select_action(observation, info)\n",
    "        observation, reward, truncated, terminated, info = local_env.step(action_int)\n",
    "        done = terminated or truncated\n",
    "        local_agent.step(observation, reward, done, action_int)\n",
    "    return local_agent.memory.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()\n",
    "agent = Agent(env)\n",
    "losses = []\n",
    "for _ in tqdm(range(5)):\n",
    "    memories = run_play(env, agent)\n",
    "    loss = agent.optimize_model(1000)\n",
    "    losses += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [x.state for x in memories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, state in enumerate(tqdm(states)):\n",
    "    if i % 1000 == 1:\n",
    "        screen = torch.split(state, [4096, 8], dim=1)\n",
    "        plt.imshow(screen[0].view(64, 64).cpu().detach().numpy())\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokerl-PCbqR789-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
